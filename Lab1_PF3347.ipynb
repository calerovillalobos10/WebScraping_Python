{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/calerovillalobos10/WebScraping_Python/blob/main/Lab1_PF3347.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lab1 Análisis en Ciencia de Datos\n"
      ],
      "metadata": {
        "id": "ivp_vDCFiMkG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Estudiantes\n",
        "\n",
        "*   Bryan Thomas Calero Villalobos\n",
        "*   Daniela Montero Parkinson\n",
        "*   Elemento de la lista"
      ],
      "metadata": {
        "id": "SiQ0TPrkCgsk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ¿Qué es web scraping?\n",
        "\n",
        "### Definición de web scraping\n",
        "\n",
        "Según Datademia (s.f.) el web scraping se puede ver como el proceso de recolectar datos desde un sitio web, sea de manera manual o automática. De manera similar, Cloudflare (s.f.) define este técnica como el uso de un programa informático para extraer datos que fueron realizados por otro programa.Este método en escencia convierte los datos no estructurados o semiestructurados de páginas web para ser manipulados por herramientas con el fin de ser analizados.\n",
        "\n",
        "\n",
        "### Ejemplos de aplicaciones de web scraping en el mundo real\n",
        "\n",
        "Realizando una combinación de lo mencionado por Liu (2021) y Mitchell (2024) algunas herramientas o aplicaciones utilizadas para hacer web scraping son:\n",
        "\n",
        "*   *ScraperAPI*: esta herramienta tiene integración con Python y NodeJS. Además, facilita el scraping de datos de tablas HTML.\n",
        "*   *Octoparse*: no se require habilidades de programación para utilizar la herramienta y es efeciva con sitios como: ecommerce, inversiones, criptomonedas y marketing.\n",
        "*   *Parsehub*: está dirigida a analistas de datos, comercializadores, etc. Es un aplicativo visual de web scraping, en donde se pueden extraer los datos dando click en el sitio web.\n",
        "*   *Crawly*: no necesita saber de programación, pero extrae elementos limitados. Además, convierte los datos a CSV o JSON.\n",
        "*   *Scrapy*: está enfocado a desarrolladores de Python y con conocimiento de scraping. Utiliza una biblioteca asincrónica que le permite avanzar con las siguientes tareas antes de finalizar.\n",
        "*   *Import.io*: esta herramienta extrae información de una web y los organiza por conjunto de datos.\n",
        "\n",
        "\n",
        "\n",
        "### Herramientas comunes para realizar web scraping en Python\n",
        "\n",
        "### Consideraciones éticas y legales al realizar web scraping"
      ],
      "metadata": {
        "id": "GxNv6CX3Ckla"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Referencias\n",
        "\n",
        "BeautifulSoup:Richardson, L. (2021). Beautiful Soup Documentation. Recuperado de https://www.crummy.com/software/BeautifulSoup/bs4/doc/\n",
        "\n",
        "Cloudflare. (s.f.). *¿Qué es el scraping de datos?* https://www.cloudflare.com/es-es/learning/bots/what-is-data-scraping/\n",
        "\n",
        "Datademia. (s.f.). *¿Qué es el web scraping?* https://datademia.es/blog/que-es-web-scraping\n",
        "\n",
        "https://rcs.cic.ipn.mx/2015_95/Metodologias%20para%20analisis%20politico%20utilizando%20Web%20Scraping.pdf\n",
        "\n",
        "Mitchell, A. (12 de Agosto 2024). Herramientas de Web Scraping. Guru99. https://www.guru99.com/es/web-scraping-tools.html\n",
        "\n",
        "Requests:Reitz, K. (2021). Requests: HTTP for Humans. Recuperado de https://docs.python-requests.org/en/latest/\n",
        "\n",
        "Scrapy:Scrapy Project. (2024). Scrapy Documentation. Recuperado de https://docs.scrapy.org/en/latest/\n",
        "\n",
        "Liu, V. (1 de septiembre 2021). Los 30 mejores software gratuitos de web scraping en 2021. Medium. https://medium.com/@veronicaliuoctoparse/los-30-mejores-software-gratuitos-de-web-scraping-en-2021-a9c89d819e03\n",
        "\n"
      ],
      "metadata": {
        "id": "Bcf325NrDWfF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Uso práctico - Web Scraping en Gollo"
      ],
      "metadata": {
        "id": "4Ch3alwWvG0Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extracción de datos"
      ],
      "metadata": {
        "id": "DHIVnGDivSl3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Primero se instalarán las dependencias\n",
        "\n",
        "*   Se utilizará la librería **requests** para realizar la petición htpp y poder leer el documento HTML de la url brindada *https://www.gollo.com/*\n",
        "*   Se utilizará la librería **BeautifulSoup** para realizar las extracciones de las diferentes partes de interés"
      ],
      "metadata": {
        "id": "8gNzWx6yDk2B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install requests\n",
        "!pip3 install beautifulsoup4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bedm1pRLwtn4",
        "outputId": "8536307f-f61b-46c0-a9fd-af9eba18264a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.7.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se procede a importar las dependencias para poder hacer uso de las mismas"
      ],
      "metadata": {
        "id": "JVoWVjOEEK2V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup"
      ],
      "metadata": {
        "id": "8BqKuTyxW5K5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se inicializan y definen las variables a utilizar. En donde la variable *page_number* y *max_pages* se utilizan para controlar el ciclo de la paginación y *all_data* se utiliza para almacenar los datos recolectados"
      ],
      "metadata": {
        "id": "4cvnlfxrFajo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "page_number = 1 # Número de página inicial\n",
        "max_pages = 130  # Establece un límite máximo para evitar bucles infinitos\n",
        "all_data = [] # Lista para almacenar todos los datos recolectados"
      ],
      "metadata": {
        "id": "-T5895J2W7uk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Se crea un bucle para recorrer cada página del sitio web. Se construye dinámicamente la URL para cada página y se realiza una solicitud HTTP con *requests.get(url)*. Si la solicitud no es exitosa (código 200) el bucle se detiene\n",
        "\n",
        "\n",
        "```\n",
        "    while page_number <= max_pages:\n",
        "\n",
        "      # Construcción dinámica de la URL para cada página\n",
        "      url = f'https://www.gollo.com/c?p={page_number}'\n",
        "      page = requests.get(url)\n",
        "\n",
        "      # Verificación del estado de la solicitud HTTP\n",
        "      if page.status_code != 200:\n",
        "        print(f\"Error al acceder a la página {page_number}. Código de estado: {page.status_code}\")\n",
        "        break\n",
        "```\n",
        "\n",
        "2. En la siguiente porción de código se utiliza *BeautifulSoup* para analizar el HTML del sitio web. Luego se buscan los bloques que contienen los detalles de los productos y sino se encuentran productos se rompe el ciclo\n",
        "\n",
        "  * La línea *soup = BeautifulSoup(page.text, 'html.parser')* convierte el contenido HTML del sitio web en un objeto de BeautifulSoup que facilita la búsqueda, navegación y manipulación del HTML en Python. Esto permite extraer y trabajar con elementos específicos del documento de manera sencilla\n",
        "  * La línea *block_items = soup.find_all(class_='product-item-details')* busca y devuelve todos los elementos HTML del sitio que tienen la clase *product-item-details*. Usa el método *find_all()* de BeautifulSoup, que encuentra todos los elementos que coinciden con ese criterio, y los almacena en la lista block_items para su posterior análisis\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "      # Parseo del contenido HTML de la página\n",
        "      soup = BeautifulSoup(page.text, 'html.parser')\n",
        "\n",
        "      # Búsqueda de los bloques que contienen detalles de los productos\n",
        "      block_items = soup.find_all(class_='product-item-details')\n",
        "\n",
        "      # Si no hay más productos en la página, se detiene el bucle\n",
        "      if not block_items:\n",
        "        print(f\"No se encontraron más productos en la página {page_number}.\")\n",
        "        break\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "3. Se extrae y almacena los datos de cada producto en la página actual, en donde se recolecta el nombre del producto, varios precios (final, antiguo y especial) y la categoría. Esto con el fin de almacenarlo en una lista para después poner la información en un dataframe\n",
        "\n",
        "  * El método *find()* de BeautifulSoup busca y devuelve el primer elemento HTML que coincide con los criterios especificados. En donde:\n",
        "    *   *block_final_price = items.find(class_='price-final_price')*: Busca el primer elemento dentro de items que tenga la clase *price-final_price*\n",
        "    * *final_price = block_final_price.find(class_='price').text.strip() if block_final_price else None*: Si se encontró *block_final_price*, busca dentro de él el primer elemento con la clase *price* y extrae el texto. Este proceso se repite para block_old_price* y *block_special_price*, buscando elementos con las clases *old-price* y *special-price*, respectivamente\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "      # Recolección de los datos de cada producto en la página actual\n",
        "      page_data = []\n",
        "      for items in block_items:\n",
        "        # Extracción del nombre del producto\n",
        "        name = items.find(class_='product-item-link').text.strip()\n",
        "\n",
        "        # Extracción de precios finales, antiguos y especiales si están disponibles\n",
        "        block_final_price = items.find(class_='price-final_price')\n",
        "        final_price = block_final_price.find(class_='price').text.strip() if block_final_price else None\n",
        "\n",
        "        block_old_price = items.find(class_='old-price')\n",
        "        old_price = block_old_price.find(class_='price').text.strip() if block_old_price else None\n",
        "\n",
        "        block_special_price = items.find(class_='special-price')\n",
        "        special_price = block_special_price.find(class_='price').text.strip() if block_special_price else None\n",
        "\n",
        "        # Agregar los datos recolectados a la lista de la página actual\n",
        "        page_data.append({\n",
        "            \"name\": name,\n",
        "            \"final_price\": final_price,\n",
        "            \"old_price\": old_price,\n",
        "            \"special_price\": special_price,\n",
        "            \"category\": name.split()[0],\n",
        "        })\n",
        "\n",
        "      # Almacena los datos de la página actual en la lista principal\n",
        "      all_data.extend(page_data)\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "maNynHoHFfe8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "while page_number <= max_pages:\n",
        "\n",
        "  # Construcción dinámica de la URL para cada página\n",
        "  url = f'https://www.gollo.com/c?p={page_number}'\n",
        "  page = requests.get(url)\n",
        "\n",
        "  # Verificación del estado de la solicitud HTTP\n",
        "  if page.status_code != 200:\n",
        "    print(f\"Error al acceder a la página {page_number}. Código de estado: {page.status_code}\")\n",
        "    break\n",
        "\n",
        "  # Parseo del contenido HTML de la página\n",
        "  soup = BeautifulSoup(page.text, 'html.parser')\n",
        "\n",
        "  # Búsqueda de los bloques que contienen detalles de los productos\n",
        "  block_items = soup.find_all(class_='product-item-details')\n",
        "\n",
        "  # Si no hay más productos en la página, se detiene el bucle\n",
        "  if not block_items:\n",
        "    print(f\"No se encontraron más productos en la página {page_number}.\")\n",
        "    break\n",
        "\n",
        "  # Recolección de los datos de cada producto en la página actual\n",
        "  page_data = []\n",
        "  for items in block_items:\n",
        "    # Extracción del nombre del producto\n",
        "    name = items.find(class_='product-item-link').text.strip()\n",
        "\n",
        "    # Extracción de precios finales, antiguos y especiales si están disponibles\n",
        "    block_final_price = items.find(class_='price-final_price')\n",
        "    final_price = block_final_price.find(class_='price').text.strip() if block_final_price else None\n",
        "\n",
        "    block_old_price = items.find(class_='old-price')\n",
        "    old_price = block_old_price.find(class_='price').text.strip() if block_old_price else None\n",
        "\n",
        "    block_special_price = items.find(class_='special-price')\n",
        "    special_price = block_special_price.find(class_='price').text.strip() if block_special_price else None\n",
        "\n",
        "    # Agregar los datos recolectados a la lista de la página actual\n",
        "    page_data.append({\n",
        "        \"name\": name,\n",
        "        \"final_price\": final_price,\n",
        "        \"old_price\": old_price,\n",
        "        \"special_price\": special_price,\n",
        "        \"category\": name.split()[0],\n",
        "    })\n",
        "\n",
        "  # Almacena los datos de la página actual en la lista principal\n",
        "  all_data.extend(page_data)\n",
        "\n",
        "  # Avanza a la siguiente página\n",
        "  page_number += 1"
      ],
      "metadata": {
        "id": "DpWQhzEdW-30"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Procesamiento de datos"
      ],
      "metadata": {
        "id": "Ob0iJaU5vXuJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "IAVlllxna8bZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(all_data, columns=[\"name\", \"final_price\", \"old_price\", \"special_price\", \"category\"])\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PYWCzfQucVjq",
        "outputId": "70f5be48-38bc-4409-c18e-69345e546144"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                  name final_price old_price  \\\n",
            "0        Base de Colchón Herrera Matrimonial Valeriana    ₡259.000      None   \n",
            "1     Parlante JBL 90W Bluetooth Negro JBLAUTH200BLKAM    ₡199.900  ₡199.900   \n",
            "2           Celular Nokia 128MB 2.4\" Midnight Blue 110     ₡29.900      None   \n",
            "3         Televisor JVC Google LED 58\" UHD  LT-58KB538    ₡309.900  ₡309.900   \n",
            "4              Base de Colchón Vargas Individual Rombo    ₡159.900      None   \n",
            "...                                                ...         ...       ...   \n",
            "3095           Colchón Sealy Matrimonial Hybrid Silver    ₡359.000      None   \n",
            "3096               Ropero Commodity Aracaju MDP Blanco    ₡379.900      None   \n",
            "3097             Ropero Herrera Colonial Madera Tabaco    ₡265.000      None   \n",
            "3098              Ropero Herrera Clásico Madera Tabaco    ₡315.000  ₡315.000   \n",
            "3099     Ropero Herrera Chifonier Madera de pino Negro    ₡345.000      None   \n",
            "\n",
            "     special_price   category  \n",
            "0             None       Base  \n",
            "1         ₡169.895   Parlante  \n",
            "2             None    Celular  \n",
            "3         ₡249.905  Televisor  \n",
            "4             None       Base  \n",
            "...            ...        ...  \n",
            "3095          None    Colchón  \n",
            "3096          None     Ropero  \n",
            "3097          None     Ropero  \n",
            "3098      ₡285.485     Ropero  \n",
            "3099          None     Ropero  \n",
            "\n",
            "[3100 rows x 5 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualización de datos"
      ],
      "metadata": {
        "id": "w0W1bw3jvbYk"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rzq57Ia7DPgf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}